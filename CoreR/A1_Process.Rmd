---
title:  "A1_Process.Rmd / Heat-Related Mortality / Exposure Response"
author: "Hans-Aloys Wischmann"
date:   "Written: 2023-04-12 to 2023-08-31 / Totally revised: 2024-11-07 to 2024-11-29 / Executed: `r Sys.Date()`"
output: word_document
---

```{r setup, include = FALSE}
  # ensure consistency across systems, define presets, set default figure size
  Sys.setlocale("LC_ALL", 'en_US.UTF-8')
  Sys.setenv(LANG = "en_US.UTF-8")
  knitr::opts_chunk$set(echo = FALSE, dpi = 1200, fig.width = 10.0, fig.height = 10.0)
  knitr::opts_knit$set(root.dir = getwd())

  # create a clean slate
  rm(list = ls())

  # get initial time stamp
  start_time <- Sys.time()

  # configurable parameters: years to use for modeling and prediction
  YEARS_MODEL <- 2000:2023            # include COVID pandemic years, but with exclusions
  COVID_YEARS <- c(2020, 2022)        # two years with COVID mortality impact during summer
  LOADS_YEAR  <- min(COVID_YEARS) - 1 # show factor loading just before COVID

  # configurable parameters: seasonal week range to analyze (summer)
  FIRST_SUMMER_WEEK <- 15 # as in RKI model: begin of summer period
  LAST_SUMMER_WEEK  <- 40 # as in RKI model: end of summer period
  DOF_TMPC          <-  8 # as in RKI model: degrees of freedom for dose-response curves
  DOF_WEEK          <-  5 # fewer than in RKI model
  LAG_MAX           <-  3 # maximum number of days to look back for temperature exposure

  # configurable parameters: countries and NUTS levels to process
  countries_of_interest <- c("UK" = "United Kingdom", "FR" = "France", "DE" = "Germany", "IT" = "Italy", "ES" = "Spain",
                             "PT" = "Portugal", "BE" = "Belgium", "NL" = "Netherlands", "CH" = "Switzerland", "AT" = "Austria")
  levels_of_interest    <- c("UK" = 1, "FR" = 2, "DE" = 1, "IT" = 2, "ES" = 2, "PT" = 2, "BE" = 2, "NL" = 2, "CH" = 2, "AT" = 2)

  # configurable parameters: geographies to exclude (regular expression)
  geographies_to_exclude = "^FRY|^FRX|^ES7|^ES63|^ES64|^UKN|^UKM|^PT2|^PT3"

  # cutoff for statistical significance
  ALPHA <- 0.05

  # document configurable parameters
  params_config <- mget(ls())
  save(params_config, file = "../Processed/Parameters.Rdata")

  # define color palettes for categorical data with 12 entries
  safe_colorblind_palette <- c("#88CCEE", "#CC6677", "#DDCC77", "#117733", "#332288", "#AA4499", "#44AA99", "#999933", "#882255", "#661100", "#6699CC", "#888888")
  safe_heatmap_palette    <- c("#0066FF", "#00CCFF", "#66FF00", "#FFFF00", "#FF7F00", "#FF0000", "#CF00FF", "#9F00FF")

  # function to replace install.packages/library combination
  library_wrapper <- function(package) {
    if (!require(package, character.only = TRUE)) {
      install.packages(package)
    }
    library(package, character.only = TRUE)
    message(paste("Using package:", package, "version", packageVersion(package)))
  }

  # load libraries, after installing if necessary
  library_wrapper("tidyverse")
  library_wrapper("readxl")
  library_wrapper("foreach")
  library_wrapper("doParallel")
  library_wrapper("ISOweek")
  library_wrapper("ncdf4")
  library_wrapper("mgcv")
  library_wrapper("sf")
  library_wrapper("scales")
  library_wrapper("lubridate")
  library_wrapper("flextable")
  library_wrapper("splines")
  library_wrapper("RhpcBLASctl")

  # use all physical cores when processing in parallel, but limit BLAS to two threads per parallel chunk
  registerDoParallel(cores = get_num_cores())
  blas_set_num_threads(get_num_procs() / get_num_cores())

  # read/write csv and tsv files in "UTF-8" encoding with semi-colon as separator, without row numbers
  load_csv <- function(file) { return(read.csv(file, sep = ";", encoding = "UTF-8")) }
  load_tsv <- function(file) { return(read.csv(file, sep = "\t", encoding = "UTF-8")) }

  # utility function to plot to *.pdf and *.png files and inline
  ggplot_font_size = 10 # font size for all texts except for geom_text, in points
  update_geom_defaults("text", list(size = ggplot_font_size * 0.35)) # font size for geom_text, in mm
  plot_pdf_png <- function(file_name, aspect_ratio, plot_object, plot_width = 8.0, plot_res = 1200) {
    themed_object <- plot_object + theme(text = element_text(size = ggplot_font_size), plot.title = element_text(size = ggplot_font_size))
    pdf_file <- sprintf("../Plots/%s.pdf", file_name)
    pdf(pdf_file, plot_width, plot_width * aspect_ratio, paper = "a4")
    print(themed_object)
    ignore <- dev.off()
    png_file <- sprintf("../Plots/%s.png", file_name)
    png(png_file, width = plot_width, height = plot_width * aspect_ratio, units = "in", res = plot_res)
    print(themed_object)
    ignore <- dev.off()
    knitr::include_graphics(png_file, dpi = 1200)
  }

  # create a standard theme
  std_theme <- function() {
    theme(
      panel.border     = element_rect(colour = "black", fill = NA),
      panel.background = element_rect(fill   = "white"),
      panel.grid.major = element_line(colour = "gray80", linewidth = 0.2),
      axis.text  = element_text(colour = "black"),
      axis.title = element_text(colour = "black"),
      axis.ticks = element_line(colour = "black"),
      legend.position = "right"
    )
  }

  # week lookup table
  FIRST_WEEK <- sprintf("%d-W01-1", min(YEARS_MODEL))
  LAST_WEEK  <- sprintf("%d-W52-1", max(YEARS_MODEL))
  week_table <- data.frame(date_from = seq(ISOweek2date(FIRST_WEEK), ISOweek2date(LAST_WEEK), 7)) %>%
    mutate(date_to = date_from + 6, year = isoyear(date_from), week = isoweek(date_from))

  # new year and mid year tick marks for multi-year plots, axis labels
  MID_YEAR_DATES <- seq(as.Date(sprintf("%d-07-01", min(YEARS_MODEL))),
                        as.Date(sprintf("%d-07-01", max(YEARS_MODEL))), by = "year")
  NEW_YEAR_DATES <- seq(as.Date(sprintf("%d-01-01", min(YEARS_MODEL))),
                        as.Date(sprintf("%d-01-01", max(YEARS_MODEL) + 1)), by = "year")
  YEAR_BREAKS <- c(MID_YEAR_DATES, NEW_YEAR_DATES)
  YEAR_LABELS <- c(YEARS_MODEL, rep("|", length(NEW_YEAR_DATES)))

  # age ranges (standard plus coarse to match available data), plus mapping for aggregation
  age_5y_standard <- data.frame(age_from = seq(0, 85, 5),    age_to = c(seq(4, 84, 5), 99)) %>%
    mutate(age = fct_inorder(paste(age_from, age_to, sep = "-")))
  age_4cat_coarse <- data.frame(age_from = c(0, 65, 75, 85), age_to = c(64, 74, 84, 99))    %>%
    mutate(age = fct_inorder(paste(age_from, age_to, sep = "-")))
  age_aggregate   <- left_join(age_5y_standard, age_4cat_coarse, join_by("age_from" >= "age_from", "age_to" <= "age_to")) %>%
    select(age = age.x, age_cat = age.y)
```

## Prerequisites

A0_Download.sh

## Expected Execution Time

About 12 min @ 12 physical cores, 24 logical cores, 32 GB RAM

## Geography

Acknowledgement for administrative boundaries: © EuroGeographics for the administrative boundaries.

Populations are available separately for England and Wales, so that they need to be treated as separate countries for the pre-processing.

```{r NUTS}
  # read NUTS map / geography provided by EU commission, note required acknowledgement (above)
  geo_info <- data.frame(read_sf("../Downloads/Geography/NUTS_RG_01M_2021_4326.shp")) %>%
    filter(LEVL_CODE %in% 1:2, str_sub(NUTS_ID, 1, 2) %in% names(countries_of_interest)) %>%             # 1st to 2nd level
    select(level = LEVL_CODE, id_region = NUTS_ID, region = NUTS_NAME, geometry) %>%                     # discard irrelevant info
    mutate(region = ifelse(str_sub(region, 1, 6) != "Centre", region, "Centre-Val de Loire")) %>%        # fix nasty hyphen
    mutate(region = ifelse(str_sub(region, 1, 6) != "Proven", region, "Provence-Alpes-Côte d'Azur")) %>% # fix nasty apostrophe
    mutate(country = str_replace_all(str_sub(id_region, 1, 2), countries_of_interest)) %>%               # assign country to each geo
    mutate(country = ifelse(str_sub(id_region, 1, 2) == "UK",  "England", country)) %>%                  # treat England as a country
    mutate(country = ifelse(str_sub(id_region, 1, 3) == "UKL", "Wales",   country)) %>%                  # treat Wales as a country
    arrange(country, region) %>%
    filter(level == levels_of_interest[str_sub(id_region, 1, 2)])
  geo_info %>% select(id_region, region) %>%
    filter(grepl(geographies_to_exclude, id_region)) %>%
    arrange(id_region) %>%
    flextable() %>% set_caption("Excluded regions") %>% autofit()
  geo_info <- geo_info %>% filter(!grepl(geographies_to_exclude, id_region)) 
  save(geo_info, file = "../Processed/Geography.Rdata", compress = "xz")

  # process 5-digit post / zip codes for France, for mapping individual death record locations to NUTS3 regions
  zip_codes_fr <- load_csv("../Downloads/Geography/pc2020_FR_NUTS-2021_v2.0.csv") %>%
    mutate(id_region = str_replace_all(NUTS3, "'", ""), zip = as.integer(str_replace_all(CODE, "'", "")) %/% 1000) %>%
    group_by(id_region) %>% reframe(zip = median(zip)) %>% arrange(zip) %>% filter(zip <= 95) %>% # metropolitan France only
    mutate(id_region = str_sub(id_region, 1, 2 + levels_of_interest["FR"])) %>%
    left_join(., geo_info, by = "id_region") %>%
    select(zip, id_region) %>%
    mutate(zip = case_when(id_region == "FRM01" ~ "2A", id_region == "FRM02" ~ "2B", TRUE ~ as.character(zip))) %>%
    distinct()
  save(zip_codes_fr, file = "../Processed/ZIP_France.Rdata", compress = "xz")

  # number of regions per country
  geo_info %>% select(id_region, Country = country) %>%
    mutate(Country = ifelse(Country %in% c("England", "Wales"), "England.Wales", Country)) %>%
    group_by(Country) %>% reframe(n = n()) %>%
    bind_rows(reframe(., Country = paste("Total, N=", n(), sep = ""), n = sum(n))) %>%
    flextable() %>% set_caption("Number of Regions per Country") %>% hline(i = 10) %>% autofit()
```

## Populations: Eurostat

© European Union, 1995-2024

Unless otherwise indicated (e.g. in individual copyright notices), content owned by the EU on this website is licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) licence.

This means that reuse is allowed, provided appropriate credit is given and changes are indicated

```{r population_eurostat}
  # string to convert age range texts to age ranges
  ages_replace <- c("Y_LT1" = "0", "Y_LT5" = "0-4", "Y_GE85" = "85-99", "Y" = "")
  ages_ignore  <- c("UNK", "TOTAL", "Y_GE75", "Y_GE80", "Y_GE90", "Y_OPEN")

  # read NUTS2 level populations statistics file from Eurostat
  population_eurostat <- load_tsv("../Downloads/Population/Population_NUTS2.tsv") %>%
    rename("label" = "freq.unit.sex.age.geo.TIME_PERIOD") %>%
    separate_wider_delim(label, names = c("ignore1", "ignore2", "sex", "age", "geo"), delim = ",") %>%
    filter(sex %in% c("F", "M"), !age %in% ages_ignore, geo %in% unique(geo_info$id_region), str_sub(geo, 1, 2) != "UK") %>%
    mutate(country = as.character(countries_of_interest[str_sub(geo, 1, 2)])) %>%
    select(geo, country, sex, age, starts_with("X20")) %>%
    pivot_longer(starts_with("X"), names_to = "year", values_to = "n_pop") %>%
    mutate(age = str_replace_all(age, ages_replace)) %>%
    filter(!grepl(":", n_pop)) %>%
    mutate(year = as.double(str_sub(year, 2, -1)), n_pop = as.integer(gsub(" .*$", "", n_pop)))

  # manually correct missing/broken entry that could be recovered/computed for Male from Total - Female
  population_eurostat <-
    rbind(population_eurostat, data.frame(geo = "ES21", country = "Spain", sex = "M", age = "45-49", year = 2001, n_pop = 71911))
  population_eurostat %>% filter(geo == "ES21", year == 2001, age == "45-49") %>%
    select(Region = geo, Country = country, Sex = sex, Age = age, Year = year, n_pop) %>%
    flextable() %>% set_caption("Missing n_pop Data for Males from Total - Females")
```

## Populations: ONS

ONS Crown Copyright Reserved [from Nomis on 25 June 2024]

```{r population_ONS}
  # read NOMIS query results files, excluding Scotland and Northern Ireland
  population_region_ONS <- load_tsv("../Downloads/Population/Population_England_Wales_NUTS1.tsv") %>%
    filter(!region %in% c("Scotland", "Northern Ireland")) %>%
    mutate(sex = str_sub(Gender, 1, 1), age = str_replace_all(Age, c("Age " = "", "Aged " = "", " " = "", "85[+]" = "85-99"))) %>%
    left_join(., geo_info %>% select(id_region, region) %>%
    mutate(region = str_replace_all(region, c(" [(]England[)]" = "", " of England" = "", "the" = "The"))), by = "region") %>%
    mutate(country = ifelse(region == "Wales", "Wales", "England")) %>%
    select(geo = id_region, country, sex, age, year = date, n_pop = value)
```

## Populations: Aggregate and Save

```{r populations}
  # combine Eurostat and ONS populations data
  population_5y <- rbind(population_eurostat, population_region_ONS)

  # aggregated to 4 coarse brackets
  population_4cat <- left_join(population_5y, age_aggregate, by = "age") %>%
    group_by(geo, country, sex, year, age = age_cat) %>% reframe(n_pop = sum(n_pop))

  # save and clean up
  save(population_5y, population_4cat, file = "../Processed/Populations.Rdata", compress = "xz")
  print(sprintf("Populations.Rdata (is.na = %d)", sum(is.na(population_5y)) + sum(is.na(population_4cat))))
  rm(list = ls(pattern = "populations"))
  load(file = "../Processed/Populations.Rdata")
```

## Quick visualizations: Populations

```{r visualize_population_pyramids}
  # quick visual inspection
  plot_pdf_png("Figure_S1", aspect_ratio = 1.0,
    population_5y %>% filter(year == 2022) %>% mutate(n_pop = ifelse(sex == "F", -n_pop, n_pop)) %>%
    left_join(., age_5y_standard, by = "age") %>% arrange(age_from) %>% mutate(age = fct_inorder(age)) %>%
    ggplot(aes(x = age, y = n_pop, fill = sex)) + geom_col() +
      scale_y_continuous(labels = ~abs(.x)) +
      scale_fill_discrete(name = "Sex") +
      labs(x = "Age [years]", y = "") + coord_flip() + 
      facet_wrap(~country, scale = "free_x") +
      std_theme() + theme(legend.position = "bottom"))
  print("2022: Population Data © European Union, 1995-2024 and ONS, 2024")
```

# Deaths (Eurostat)

© European Union, 1995-2024

Unless otherwise indicated (e.g. in individual copyright notices), content owned by the EU on this website is licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) licence.

This means that reuse is allowed, provided appropriate credit is given and changes are indicated

```{r deaths_eurostat}
  # string to convert age range texts to age ranges
  ages_replace <- c("Y_LT1" = "0", "Y_LT5" = "0-4", "Y_GE85" = "85-99", "Y_GE90" = "90-99", "Y" = "")
  ages_ignore  <- c("UNK", "TOTAL")

  # read NUTS2 level weekly death statistics file from Eurostat
  deaths_eurostat <- load_tsv("../Downloads/Population/Deaths_Weekly_NUTS2.tsv")  %>% 
    # separate concatenated variables from label column
    rename("label" = "freq.age.sex.unit.geo.TIME_PERIOD") %>%
    separate_wider_delim(label, names = c("ignore1", "age", "sex", "ignore2", "geo"), delim = ",") %>%
    # use data by sex, ignore deaths at unknown age, select only geographies of interest
    filter(sex %in% c("F", "M"), !age %in% ages_ignore, geo %in% unique(geo_info$id_region), !str_sub(geo, 1, 2) %in% c("UK", "FR", "DE")) %>%
    mutate(age = str_replace_all(age, ages_replace)) %>%
    # keep only relevant info, in correct order
    select(geo, sex, age, starts_with("X20")) %>%
    # convert to tidy format
    pivot_longer(starts_with("X"), names_to = "year_week", values_to = "n_deaths") %>%
    # convert year.week into start date of week
    mutate(year_week = str_replace_all(year_week, c("X" = "", "[.]" = "-", "$" = "-1")), date_from = ISOweek2date(year_week)) %>%
    # remove rows with missing data and convert number of deaths to integer
    filter(!grepl(":", n_deaths)) %>% mutate(n_deaths = as.integer(gsub(" .*$", "", n_deaths))) %>%
    mutate(country = as.character(countries_of_interest[str_sub(geo, 1, 2)])) %>%
    select(geo, country, sex, age, date_from, n_deaths)
```

# Deaths (DESTATIS)

© Statistisches Bundesamt (Destatis), 2024

Vervielfältigung und Verbreitung, auch auszugsweise, mit Quellenangabe gestattet.

```{r deaths_DESTATIS}
  # Read two different files and two separate tabs in each file, one for each sex
  deaths_2000_2019_m <- read_excel("../Downloads/Population/Deaths_Germany_2000_2019.xlsx", sheet = "csv-12613-10", col_types = "text")
  deaths_2000_2019_f <- read_excel("../Downloads/Population/Deaths_Germany_2000_2019.xlsx", sheet = "csv-12613-11", col_types = "text")
  deaths_2020_2023_m <- read_excel("../Downloads/Population/Deaths_Germany_2020_2023.xlsx", sheet = "csv-12613-10", col_types = "text")
  deaths_2020_2023_f <- read_excel("../Downloads/Population/Deaths_Germany_2020_2023.xlsx", sheet = "csv-12613-11", col_types = "text")
  deaths_DESTATIS <- rbind(deaths_2000_2019_m, deaths_2000_2019_f, deaths_2020_2023_m, deaths_2020_2023_f) %>% select(!Statistik)

  # combine four tables, all with same columns, select 2000-2023 years
  deaths_DESTATIS <- deaths_DESTATIS  %>% distinct() %>%
    filter(Jahre %in% YEARS_MODEL, Alter != "Insgesamt") %>%
    mutate(age = str_replace(Alter, "85[+]", "85-99"), sex = ifelse(Geschlecht == "Weiblich", "F", "M")) %>%
    mutate(year_week = sprintf("%04d-W%02d-1", as.integer(Jahre), as.integer(Kalenderwoche)), date_from = ISOweek2date(year_week)) %>%
    left_join(., geo_info  %>% select(id_region, region, country), join_by("Gebiet" == "region")) %>%
    select(geo = id_region, country, sex, age, date_from, n_deaths = Sterbefaelle) %>%
    filter(!n_deaths %in% c("X", ".")) %>% mutate(n_deaths = as.integer(n_deaths))
```

## Deaths (ONS, Commissioned)

```{r deaths_ONS}
  # ONS uses region codes, NUTS requires the long version, conversion vector
  region_strings <- c("E12000001" = "North East (England)", "E12000002" = "North West (England)", "E12000003" = "Yorkshire and the Humber",
                      "E12000004" = "East Midlands (England)", "E12000005" = "West Midlands (England)",  "E12000006" = "East of England",
                      "E12000007" = "London", "E12000008" = "South East (England)", "E12000009" = "South West (England)", "W92000004" = "Wales")

  # deaths by week of occurrence, by region, by sex, and by five-year age group
  deaths_ONS <-
    read_xlsx("../Downloads/Population/Deaths_England_Wales_1981_2022.xlsx", sheet = "1", skip = 5) %>%
    rename("date_from" = "Week start date\r\n(Saturday)") %>% select(!starts_with("Week")) %>%
    mutate(sex = ifelse(Sex == "1", "M", "F")) %>% # as specified in Notes sheet inside the file
    pivot_longer(starts_with(c("E", "W")), names_to = "region_age", values_to = "n_deaths") %>%
    mutate(region_code = gsub("_.*", "", region_age), age_range = gsub(".*_", "", region_age)) %>%
    mutate(region = str_replace_all(region_code, region_strings)) %>%
    mutate(age = str_replace_all(age_range, c("<1" = "0-4", "95[+]" = "95-99", "01-04" = "0-4", "05-09" = "5-9"))) %>%
    left_join(., geo_info  %>% select(id_region, region, country), by = "region") %>%
    filter(date_from >= week_table$date_from[1] - 2) %>%
    group_by(geo = id_region, country, sex, age, date_from) %>% reframe(n_deaths = sum(n_deaths))
```

## Deaths (INSEE)

```{r deaths_INSEE}
  # read all annual and monthly death record files
  file_names <- list.files(path = "../Downloads/Population/", pattern = "Deaths_France_", full.names = TRUE, recursive = FALSE)
  deaths_raw <- foreach (i = 1:length(file_names), .combine = "rbind", .packages = "tidyverse") %dopar% {
    load_csv(file_names[i]) %>% select(sex = sexe, datenaiss, datedeces, lieunaiss, lieudeces) %>%
      mutate(sex = ifelse(sex == 1, "M", "F")) %>%
      mutate(lieunaiss = as.character(lieunaiss), lieudeces = as.character(lieudeces)) %>% filter(lieudeces != "") %>%
      mutate(lieudeces = str_replace_all(lieudeces, c("2A" = "20", "2B" = "20"))) %>%
      mutate(datenaiss = as.Date(as.character(datenaiss), format = "%Y%m%d")) %>% filter(!is.na(datenaiss))  %>%
      mutate(datedeces = as.Date(as.character(datedeces), format = "%Y%m%d")) %>% filter(!is.na(datedeces))  %>%
      mutate(age_at_death = as.integer(difftime(datedeces, datenaiss, units = "days")) %/% 365.25) %>% filter(age_at_death %in% 0:120) %>%
      mutate(age_at_death = pmin(age_at_death, 99)) %>%
      mutate(zip = as.integer(lieudeces) %/% 1000) %>% filter(zip > 0, zip < 96)
  }

  # clean up records, remove full duplicates (noting that distinct is much faster than unique)
  deaths_unique <- distinct(deaths_raw) %>% select(sex, age_at_death, datedeces, zip) %>% mutate(zip = as.character(zip))

  # exclude records for deaths that occurred in prior years, retain only deaths from FIRST_WEEK to LAST_WEEK
  deaths_INSEE <- deaths_unique %>%
    filter(datedeces >= min(week_table$date_from), datedeces <= max(week_table$date_to)) %>%
    left_join(., zip_codes_fr, by = "zip") %>%
    left_join(., age_5y_standard, join_by("age_at_death" >= "age_from", "age_at_death" <= "age_to")) %>%
    left_join(., week_table, join_by("datedeces" >= "date_from", "datedeces" <= "date_to")) %>%
    group_by(geo = id_region, sex, age, date_from) %>% reframe(n_deaths = n()) %>%
    complete(geo, sex, age, date_from, fill = list(n_deaths = 0)) %>% mutate(country = "France")
```

## Deaths: Aggregate and Save

```{r deaths}
  # combine deaths into one tidy table, combine all 85+ groups into one age category, calculate year, append population info
  deaths_5y <- rbind(deaths_eurostat, deaths_ONS, deaths_INSEE) %>%
    mutate(age = str_replace_all(age, c("85-89" = "85-99", "90-94" = "85-99", "95-99" = "85-99", "90-99" = "85-99"))) %>%
    group_by(geo, country, sex, age, date_from) %>% reframe(n_deaths = sum(n_deaths))

  # aggregate to 4 categories, append Germany
  deaths_4cat <- left_join(deaths_5y, age_aggregate, by = "age") %>%
    group_by(geo, country, sex, age = age_cat, date_from) %>% reframe(n_deaths = sum(n_deaths)) %>%
    rbind(., deaths_DESTATIS)

  # save and clean up
  save(deaths_5y, deaths_4cat, file = "../Processed/Deaths.Rdata", compress = "xz")
  print(sprintf("Deaths.Rdata (is.na = %d)", sum(is.na(deaths_5y)) + sum(is.na(deaths_4cat))))
  rm(list = ls(pattern = "deaths"))
  load(file = "../Processed/Deaths.Rdata")
```

## Populations: 1km x 1km grid, accumulate onto 0.1 x 0.1 degree grid (inside regions)

© Eurostat, 1995 - today

```{r population_distribution}
  # names of downloaded files
  grid_shape_file <- "../Downloads/Geography/JRC_POPULATION_2018.shp"

  # read file with shapes and population density at 1km by 1km resolution, convert to longitude/latitude
  population_density <- read_sf(grid_shape_file) %>% filter(CNTR_ID %in% names(countries_of_interest)) %>%
    st_transform(crs = 4326) %>% select(geometry, pop = TOT_P_2018)

  # compute centroid for each polygon (1km by 1km)
  centroids <- data.frame(st_coordinates(st_centroid(population_density$geometry)))
  population_density <- population_density %>% mutate(long = centroids$X, lat = centroids$Y) %>% select(!geometry)
  rm(list = ls(pattern = "centroids"))

  # find region that each populated node (1km x 1km grid) falls within
  st_p <- st_as_sf(x = population_density, coords = c("long", "lat"), crs = 4326)
  w_in <- map_int(st_within(st_p, geo_info$geometry), ~ifelse(length(.x) > 0, .x[1], NA))
  populated_grid <- population_density %>% mutate(region = geo_info$id_region[w_in]) %>% drop_na() %>%
    mutate(long = 0.10 * floor(10.0 * long) + 0.05, lat = 0.10 * floor(10.0 * lat) + 0.05) %>%
    group_by(geo = region, long, lat) %>% reframe(pop = sum(pop))
  save(populated_grid, file = "../Processed/Populated_Grid.Rdata", compress = "xz")
  rm(list = ls(pattern = "st_p"))

  # plot populated grid for visual check
  plot_pdf_png("Figure_S2", aspect_ratio = 1.2,
    populated_grid %>%
    group_by(long, lat) %>% reframe(pop = pmin(pmax(log10(sum(pop)), 2.5), 5.5)) %>%
    mutate(long_min = long - 0.05, long_max = long + 0.05, lat_min = lat - 0.05, lat_max = lat + 0.05) %>%
    ggplot(aes(xmin = long_min, xmax = long_max, ymin = lat_min, ymax = lat_max, fill = pop)) +
      geom_sf(data = geo_info$geometry, inherit.aes = FALSE, aes(), linewidth = 0.1, color = "#00007F", fill = "#00007F") +
      geom_rect() +
      labs(fill = "Population",
           y = "© Eurostat, EFGS, for the populations density grid",
           x = "© EuroGeographics for the administrative boundaries") +
      scale_fill_gradient(low = "#00007F", high = "#FFFF7F", breaks = 3:6, labels = ~sprintf("%d", 10^(3:6)), limits = c(2.5, 6.0)) +
      geom_sf(data = geo_info$geometry, inherit.aes = FALSE, aes(), linewidth = 0.2, color = "white", fill = NA) +
      std_theme())
  print("2018 Population (0.1 ° x 0.1 ° Grid)", fill = "Population")
```

## Daily Mean Temperatures

Do not attempt to run this chunk with less than 16 GB of RAM!

Acknowledgment: We acknowledge the E-OBS dataset from the EU-FP6 project UERRA (https://www.uerra.eu) and the Copernicus Climate Change Service, and the data providers in the ECA&D project (https://www.ecad.eu)

Citation: Cornes, R., G. van der Schrier, E.J.M. van den Besselaar, and P.D. Jones. 2018: An Ensemble Version of the E-OBS Temperature and Precipitation Datasets, J. Geophys. Res. Atmos., 123. doi:10.1029/2017JD028200

```{r daily_mean_temperature_eobs}
  # names of downloaded temperature file
  mean_file <- "../Downloads/Temperature/tg_ens_mean_0.1deg_reg_v30.0e.nc"
  date_from <- min(week_table$date_from) - LAG_MAX - 2 # ONS week starts on Saturday prior to Monday of ISO week

  # read temperature grid file for Europe (ensemble mean, all longitudes, all latitudes, since 1950-01-01)
  nc_file  <- nc_open(mean_file)
  long     <- 0.10 * floor(10.0 * ncvar_get(nc_file, "longitude")) + 0.05 # longitude [°] -> center
  lat      <- 0.10 * floor(10.0 * ncvar_get(nc_file, "latitude"))  + 0.05 # latitude [°]  -> center
  idx_date <- as.integer(date_from - as.Date("1950-01-01")) + 1
  date_nc  <- as.Date("1950-01-01") + ncvar_get(nc_file, "time", start = idx_date, count = -1)

  # block read all temperatures at once (30 seconds, avoids 4.5 hours of poking in the file)
  tmp <- ncvar_get(nc_file, "tg", start = c(1, 1, idx_date), count = c(-1, -1, -1), raw_datavals = TRUE)
  tmpc_scale_factor <- ncatt_get(nc_file, "tg", "scale_factor")$value
  tmpc_fill_value   <- ncatt_get(nc_file, "tg", "_FillValue")$value
  tmpc_missing      <- tmpc_fill_value * tmpc_scale_factor
  nc_close(nc_file)
  rm(nc_file)

  # efficiently determine index (in temperature grid) for all elements of populations grid
  long_lookup <- data.frame(long, long_index = 1:length(long))
  lat_lookup  <- data.frame(lat,  lat_index  = 1:length(lat))
  populated_grid_with_lookup <- left_join(left_join(populated_grid, long_lookup, by = "long"), lat_lookup, by = "lat")

  # loop in parallel over regions, compute population_weighted mean temperature
  regions <- unique(populated_grid_with_lookup$geo)
  tmpc_mean <- foreach (i = 1:length(regions), .combine = "cbind", .packages = "tidyverse") %dopar% {
    grid_region <- populated_grid_with_lookup %>% filter(geo == regions[i])
    sum_temp <- rep(0.0, dim(tmp)[3])
    sum_pop  <- rep(0.0, dim(tmp)[3])
    for (j in 1:nrow(grid_region)) {
      idx_long <- grid_region$long_index[j]
      idx_lat  <- grid_region$lat_index[j]
      pop      <- grid_region$pop[j]
      tmpc_point <- tmpc_scale_factor * as.vector(tmp[idx_long,idx_lat,])
      sum_temp   <- sum_temp + ifelse(tmpc_point > tmpc_missing, pop * tmpc_point, 0.0)
      sum_pop    <- sum_pop  + ifelse(tmpc_point > tmpc_missing, pop,              0.0)
    }
    tmpc_mean <- data.frame(tmp = round(sum_temp / sum_pop, 1))
    names(tmpc_mean) <- grid_region$geo[1]
    tmpc_mean
  }

  # convert to tidy table
  temperatures <- cbind(date_nc, tmpc_mean) %>% pivot_longer(!date_nc, names_to = "geo", values_to = "tmp") %>% rename("date" = "date_nc")

  # compute all lagged temperatures, adding average over increasing number of prior days
  temperatures_daily_lagged = expand.grid(lag = 0:LAG_MAX, date_0 = date_nc, geo = unique(geo_info$id_region)) %>%
    mutate(date = date_0 - lag) %>%
    left_join(., temperatures, by = c("geo", "date")) %>%
    select(!date) %>% rename("date" = "date_0") %>%
    pivot_wider(values_from = "tmp", names_from = "lag", names_prefix = "tmpc_lag_") %>%
    filter(complete.cases(.)) %>%
    mutate(tmpc_lag_0_3 = round((tmpc_lag_0 + tmpc_lag_1 + tmpc_lag_2 + tmpc_lag_3) * 0.25, 1)) %>%
    select(geo, date, starts_with("tmpc_lag"))

  # compute all lagged temperatures for all days-of-the-week for all weeks
  temperatures_weekly_lagged <- expand.grid(geo = unique(geo_info$id_region), date_from = unique(deaths_4cat$date_from), day_of_week = 0:6) %>%
    mutate(date = date_from + day_of_week) %>%
    left_join(., temperatures_daily_lagged, by = c("geo", "date")) %>%
    filter(complete.cases(.)) %>%
    select(!date) %>%
    pivot_longer(starts_with("tmp"), names_to = "lag", values_to = "tmp") %>%
    mutate(variable = sprintf("%s_dow_%s", lag, day_of_week)) %>% select(geo, date_from, variable, tmp) %>%
    pivot_wider(values_from = "tmp", names_from = "variable")

  # save temperatures for all days and all weeks
  save(temperatures_daily_lagged, temperatures_weekly_lagged, file = "../Processed/Temperatures.Rdata", compress = "xz")
  print(sprintf("Temperatures.Rdata (is.na = %d)", sum(is.na(temperatures_daily_lagged)) + sum(is.na(temperatures_weekly_lagged))))
  rm(list = ls(pattern = "temperatures"))
  load("../Processed/Temperatures.Rdata")

  # count consecutive number of days > 25 °C by region using run length encoding (rle)
  consecutive_days_25 <- temperatures_daily_lagged %>%
    arrange(geo, date) %>%
    mutate(warm = tmpc_lag_0 >= 25.0) %>%
    group_by(geo) %>%
    reframe(length = rle(warm)$lengths, values = rle(warm)$values) %>%
    filter(values == TRUE, length >= 3) %>%  # at least three consecutive days
    group_by(geo, length) %>%
    reframe(n = n()) %>%
    filter(length <= 21) %>%
    left_join(., geo_info %>% select(id_region, country), join_by("geo" == "id_region")) %>%
    group_by(country, length) %>% reframe(n = sum(n))

  plot_pdf_png("Figure_S3", aspect_ratio = 0.5,
    consecutive_days_25 %>%
    ggplot(aes(x = length, y = n, group_by = country, color = country, label = country)) +
      geom_point(size = 3.0) + geom_line(linewidth = 1.0) +
      scale_x_continuous(breaks = c(3, 7, 14, 21), trans = "log") +
      scale_color_manual(values = safe_colorblind_palette) +
      labs(y = "Cumulative Number of Stretches (across Regions)", x = "Consecutive Days >= 25 °C Mean Temperature") +
    std_theme() +
    theme(legend.position = "bottom", legend.title = element_blank()))
  print(sprintf("Hot Stretches by Region (%s to %s)", min(temperatures_daily_lagged$date), max(temperatures_daily_lagged$date)))
```

## Temperature Plots

```{r temperature_plots}
  # create example plots for heat waves of 2003, 2019, and 2023
  plot_pdf_png("Figure_S4", aspect_ratio = 0.5,
    temperatures_daily_lagged %>%
    filter(date %in% c("2003-08-12", "2019-07-25", "2023-08-24")) %>%
    pivot_longer(starts_with("tmp"), names_to = "lag", values_to = "tmp") %>%
    filter(lag == "tmpc_lag_0_3") %>%
    mutate(lag = str_replace(lag, "tmpc_lag_", "")) %>%
    separate_wider_delim(lag, names = c("lmin", "lmax"), delim = "_") %>%
    mutate(cat = sprintf("%s - %s", date - as.integer(lmax), date - as.integer(lmin))) %>%
    left_join(., geo_info, join_by("geo" == "id_region")) %>% st_as_sf() %>%
    ggplot() +
      geom_sf(aes(fill = tmp)) +
      labs(fill = "T [°C]", x = "© EuroGeographics for the administrative boundaries, © E-OBS for the temperature data") +
      theme(legend.position = "right") +
      scale_fill_gradientn(colors = safe_heatmap_palette, breaks = seq(15, 30, 5), limits = c(12.0, 33.0)) +
      geom_sf(inherit.aes = FALSE, aes(), linewidth = 0.05, color = "black", fill = NA) +
      std_theme() +
      facet_wrap(~cat, nrow = 1, dir = "h"))
  print("Average Temperatures", fill = "T [°C]")
```

## Outcome Plots: Deaths (Absolute, Standardized)

```{r deaths_plots}
  # EU-27 + EFTA Standard Population 2013, with 0-4 years combined into one group and 85+ years combined into one group, plus 4 coarse age categories
  population_eu_5y <- data.frame(age = paste(seq(0, 85, 5), c(seq(4, 84, 5), 99), sep = "-"),
                                 n_pop_eu27 = 100 * c(50, 55, 55, 55, 60, 60, 65, 70, 70, 70, 70, 65, 60, 55, 50, 40, 25, 25))
  population_eu_4cat <- left_join(population_eu_5y, age_aggregate, by = "age") %>% group_by(age = age_cat) %>% reframe(n_pop_eu27 = sum(n_pop_eu27))
  population_eu <- distinct(rbind(population_eu_5y, population_eu_4cat))

  # calculate total deaths for each country and for a standard population in each country
  deaths_total <- left_join(deaths_4cat %>% mutate(year = year(date_from)), population_4cat, by = c("geo", "country", "year", "sex", "age")) %>%
    group_by(country, year, age, date_from) %>% reframe(n_deaths = sum(n_deaths), n_pop = sum(n_pop)) %>%
    left_join(., population_eu, by = "age") %>% mutate(n_deaths_std = n_deaths * n_pop_eu27 / n_pop) %>%
    group_by(country, year, date_from) %>% reframe(n_deaths = sum(n_deaths), n_deaths_std = sum(n_deaths_std))

  # deaths (observed)
  plot_pdf_png("Figure_S5", aspect_ratio = 1.2,
    deaths_total %>% filter(year %in% YEARS_MODEL, country != "Luxembourg") %>%
    mutate(panel = sprintf("%d-%d", year - (year - min(YEARS_MODEL)) %% 8, year - (year - min(YEARS_MODEL)) %% 8 + 7)) %>%
    ggplot(aes(x = date_from, y = n_deaths, group_by = country, color = country)) +
      geom_line(linewidth = 0.5) +
      scale_x_continuous(breaks = YEAR_BREAKS, labels = YEAR_LABELS) +
      labs(color = "Country", x = "", y = "Weekly Deaths") +
      scale_color_manual(values = safe_colorblind_palette) +
      std_theme() + theme(legend.position = "top", axis.ticks.x=element_blank()) +
      guides(color = guide_legend(nrow = 2, byrow = FALSE)) +
      facet_wrap(~panel, ncol = 1, scales = "free_x"))
  print("Deaths (Observed)")

  # deaths (standardized, with mean over all weekly deaths removed by country)
  plot_pdf_png("Figure_S6", aspect_ratio = 1.2,
    deaths_total %>% filter(year %in% YEARS_MODEL, country != "Luxembourg") %>%
    group_by(country) %>% mutate(n_deaths_std = n_deaths_std - mean(n_deaths_std)) %>% ungroup() %>%
    mutate(panel = sprintf("%d-%d", year - (year - min(YEARS_MODEL)) %% 8, year - (year - min(YEARS_MODEL)) %% 8 + 7)) %>%
    ggplot(aes(x = date_from, y = n_deaths_std, group_by = country, color = country)) +
      geom_line(linewidth = 0.5) +
      scale_x_continuous(breaks = YEAR_BREAKS, labels = YEAR_LABELS) +
      labs(color = "Country", x = "", y = "Weekly Deaths - Mean(Weekly Deaths)") +
      scale_color_manual(values = safe_colorblind_palette) +
      std_theme() + theme(legend.position = "top", axis.ticks.x=element_blank()) +
      guides(color = guide_legend(nrow = 2, byrow = FALSE)) +
      facet_wrap(~panel, ncol = 1, scales = "free_x"))
  print("Deaths (EU27 Standard Population)")
```

## Exposure Response (Raw)

Exposure response is shown by country, as the sum over the regions, by temperature bracket (integer degree +/- 0.5 degrees), for pre-pandemic years.

```{r exposure_response_plot}
  # define temperature exposure for a week as the maximum over the 1-to-3 day lagged average temperatures, over the days of the week
  tmpc_exposure <- temperatures_weekly_lagged %>%
    mutate(tmpc_max_lag_0_3 = round(pmax(tmpc_lag_0_3_dow_0, tmpc_lag_0_3_dow_1, tmpc_lag_0_3_dow_2, tmpc_lag_0_3_dow_3,
                                         tmpc_lag_0_3_dow_4, tmpc_lag_0_3_dow_5, tmpc_lag_0_3_dow_6), 0)) %>%
    select(geo, date_from, tmpc_max_lag_0_3)

  # retrieve deaths per country, append population and exposure data, bucket exposure by degree, sum exposed population, compute death rate
  incremental_death_rate_by_country_exposure_by_region <-
    left_join(deaths_4cat %>% mutate(year = year(date_from)), population_4cat, by = c("geo", "country", "year", "sex", "age")) %>%
    filter(date_from >= min(week_table$date_from), year < min(COVID_YEARS)) %>%
    group_by(geo, year, sex, age) %>% mutate(n_deaths = n_deaths - mean(n_deaths)) %>% ungroup() %>%
    left_join(., tmpc_exposure, by = c("geo", "date_from")) %>%
    mutate(decade = sprintf("%d-%d", 10 * (year %/% 10), 10 * (year %/% 10) + 9)) %>%
    group_by(country, decade, age, tmpc_max_lag_0_3) %>% reframe(r_deaths = 100000.0 * sum(n_deaths) / sum(n_pop), n_pop = sum(n_pop))

  plot_pdf_png("Figure_1", aspect_ratio = 1.0,
    incremental_death_rate_by_country_exposure_by_region %>% filter(tmpc_max_lag_0_3 >= 5.0) %>%
    ggplot(aes(x = tmpc_max_lag_0_3, y = r_deaths, group_by = country, color = country, weight = n_pop)) +
      geom_smooth(aes(fill = country), formula = y ~ x, method = "loess", span = 0.7, se = TRUE, alpha = 0.2) +
      labs(color = "", fill = "",
           x = "Exposure: Mean Temperature over prior 0:3 Days, Maximum over 7 Days of Week",
           y = "Weekly Rate of Incremental Deaths [1/100k] (vs. Region-Year-Sex-Age Mean, All Regions)") +
      scale_x_continuous(breaks = seq(10, 30, 10), labels = ~sprintf("%d °C", .x)) +
      scale_fill_manual(values = safe_colorblind_palette) +
      scale_color_manual(values = safe_colorblind_palette) +
      guides(fill = guide_legend(nrow = 2, byrow = FALSE)) +
      std_theme() + theme(legend.position = "top") + facet_grid(vars(decade), vars(age), scale = "free"))
  print("Raw Exposure Response Curves, Incremental Deaths")

  plot_pdf_png("Figure_2", aspect_ratio = 1.0,
    incremental_death_rate_by_country_exposure_by_region %>% filter(tmpc_max_lag_0_3 >= 5.0) %>%
    filter(age %in% c("75-84", "85-99"), !country %in% c("Netherlands", "Italy")) %>%
    mutate(legend = sprintf("Age %s (%s)", age, decade)) %>%
    ggplot(aes(x = tmpc_max_lag_0_3, y = r_deaths, group_by = legend, color = legend, weight = n_pop)) +
      geom_smooth(aes(fill = legend), formula = y ~ x, method = "loess", span = 0.7, se = TRUE, alpha = 0.2) +
      labs(color = "", fill = "",
           x = "Exposure: Mean Temperature over prior 0:3 Days, Maximum over 7 Days of Week",
           y = "Weekly Rate of Incremental Deaths [1/100k] (vs. Region-Year-Sex-Age Mean, All Regions)") +
      scale_x_continuous(breaks = seq(10, 30, 10), labels = ~sprintf("%d °C", .x)) +
      scale_fill_manual(values = safe_colorblind_palette) +
      scale_color_manual(values = safe_colorblind_palette) +
      guides(fill = guide_legend(nrow = 2, byrow = FALSE)) +
      std_theme() + theme(legend.position = "top") + facet_wrap(~country, nrow = 3))
  print("Raw Exposure Response Curves, Incremental Deaths, 74-84 and 85+ Years")
```

## Combine Required Data

Combine deaths and populations, append (lagged) temperature exposures, filter on summer weeks.

Combine England and Wales into England.Wales for the main analysis.

```{r combine_geography_deaths_temperatures}
  # combine deaths and populations, adding week of year(Mon after Sat week start for UK, Wed for rest of Europe), select summer weeks only
  df_summer <- left_join(deaths_4cat %>% mutate(year = year(date_from)), population_4cat, by = c("geo", "country", "year", "sex", "age")) %>%
    filter(date_from >= min(week_table$date_from)) %>%
    mutate(week = gsub("^.*W", "", ISOweek(date_from + 2))) %>%
    left_join(., temperatures_weekly_lagged, by = c("geo", "date_from")) %>%
    mutate(country = ifelse(country %in% c("England", "Wales"), "England.Wales", country)) %>%
    arrange(country, geo, sex) %>%
    mutate(across(c(geo, country, sex), ~fct_inorder(.x))) %>%
    mutate(across(c(date_from, year, week, n_deaths, n_pop), ~as.integer(.x))) %>%
    filter(week >= FIRST_SUMMER_WEEK, week <= LAST_SUMMER_WEEK) %>%
    filter(date_from >= min(week_table$date_from), date_from <= max(week_table$date_from))
  save(df_summer, file = "../Processed/DeathsPopTemp.Rdata", compress = "xz")
  
  # clean up memory (end of preprocessing)
  rm(tmp, w_in)
  rm(list = ls(pattern = "tmpc"))
  rm(list = ls(pattern = "deaths"))
  rm(list = ls(pattern = "population"))
  rm(list = ls(pattern = "temperatures"))
  rm(list = ls(pattern = "populated_grid"))
  rm(list = ls(pattern = "df_summer"))
  rm(list = ls(pattern = "geo_info"))

  # make room in memory, run garbage collector
  gc(full = TRUE)

  # stop implicit clusters
  doParallel::stopImplicitCluster()
```

## Pass 1 - Individual models by region, by age, by sex, by year: Onset of heat-related risk

Individual GAMs are computed for the weekly death rates (assuming a gaussian error distribution) as a function of the mean temperature across the prior 0..3 days, separately by region(geo), by age groups, by sex, and by year. The onset (t_zero) for positive heat-related mortaliy risk is determined for each model, aggregated across all available years, and subtracted from the actual, observed temperatures, by region(geo), by age, by sex. Many individual models do not show a minimum (t_min), so that the computed estimates for the minima would be less robust and can thus not serve instead as the temperature reference. As the data frame for each model contains only 26 data points (LAST_SUMMER_WEEK - FIRST_SUMMER_WEEK + 1), the seasonal terms are modeled using a smooth function with only 5 DoF. Data from two COVID years (2020 and 2022) are excluded to avoid potential confounding by pandemic mortality effects during summer.

```{r fit_individual_GAMs}
  # reload summer data file (can restart processing here)
  load(file = "../Processed/DeathsPopTemp.Rdata")

  # fit separate models by region, by age group, by sex, by year
  do_models_1 <- df_summer %>% select(c = country, g = geo, a = age, s = sex, y = year) %>% distinct() %>% filter(!y %in% COVID_YEARS)
  do_parallel <- get_num_cores() / 2
  registerDoParallel(cores = do_parallel)
  blas_set_num_threads(get_num_procs() / do_parallel)

  df_tmpc <- foreach (i = 1:nrow(do_models_1), .combine = "rbind", .packages = c("tidyverse", "mgcv")) %dopar% {
    # extract subset of data for region, calculate weekly_death_rate for 100k population
    df_gasy <- df_summer %>% filter(geo == do_models_1$g[i], age == do_models_1$a[i], sex == do_models_1$s[i], year == do_models_1$y[i]) %>%
      mutate(weekly_death_rate = 1.0e6 * n_deaths / n_pop)

    # fit individual model with seasonal trends and a smooth term for the mean temperature across the prior 0..3 days, leveraging implicit summation over weekdays
    lag_0_3 <- as.matrix(df_gasy %>% select(starts_with("tmpc_lag_0_3_dow")))
    gam1 <- bam(weekly_death_rate ~ 1 + ti(week, k = DOF_WEEK, bs = "ps") + ti(lag_0_3, k = DOF_TMPC, bs = "ps"), data = df_gasy, family = gaussian, discrete = TRUE)

    # interpolate using identical temperature values for all lags and all weekdays (steady state), mid year for calendar weeks
    df_grid <- expand.grid(week = 26, tmpc = seq(round(min(lag_0_3), 1), round(max(lag_0_3), 1), 0.1)) %>% mutate(lag_0_3 = tmpc) %>%
      mutate(response = as.vector(predict.gam(gam1, newdata = ., type = "terms", terms = "ti(lag_0_3)")))

    # determine highest temperature for which the smooth temperature response shows a local minimum, and the highest protective/neutral temperature
    cbind(df_grid %>% filter(response <= 0) %>% reframe(t_zero = max(tmpc)),
          df_grid %>% filter(lag(response) >= response, lead(response) >= response) %>% reframe(t_min = max(tmpc)),
          country = do_models_1$c[i], geo = do_models_1$g[i], age = do_models_1$a[i], sex = do_models_1$s[i], year = do_models_1$y[i], n_pop = df_gasy$n_pop[1])
  }

  # document total number of models that were fitted
  df_tmpc %>% group_by(age, sex) %>% reframe(N = n(), ZeroCrossing = sum(t_zero != -Inf), Minima = sum(t_min != -Inf)) %>%
    bind_rows(reframe(., across(everything(), ~ifelse(is.numeric(.), sum(.), "")))) %>%
    select(Age = age, Sex = sex, N, ZeroCrossing, Minima) %>%
    flextable() %>% set_caption("Models, Detected Zero Crossings, and Detected Minima") %>% hline(i = 8) %>% autofit()

  # aggregate t_min and t_zero over years, ignoring -Inf (no minimum/no zero crossing found), with a linear trend by country alone
  lm_zero <- df_tmpc %>% filter(t_zero != -Inf) %>% lm(t_zero ~ 0 + geo:age:sex + country:year, data = ., weights = n_pop)
  lm_min  <- df_tmpc %>% filter(t_min  != -Inf) %>% lm(t_min  ~ 0 + geo:age:sex + country:year, data = ., weights = n_pop)

  # check for statistically significant slopes for t_zero (significant terms country:year)
  as.data.frame(summary(lm_zero)$coef) %>% mutate(Country = rownames(.)) %>% filter(grepl(":year", Country)) %>%
    select(Country, Estimate, SE = "Std. Error") %>% arrange(desc(Estimate)) %>%
    mutate(across(c(Estimate, SE), ~10.0 * .), Low = Estimate + qnorm(ALPHA * 0.5) * SE, High = Estimate + qnorm(1.0 - ALPHA * 0.5) * SE) %>%
    mutate(Trend = sprintf("%.2f [%.2f, %.2f] %s", Estimate, Low, High, ifelse(Low > 0 | High < 0, "*", ""))) %>%
    mutate(Country = str_replace_all(Country, c("country" = "", ":year" = ""))) %>%
    select(Country, Trend) %>%
    flextable() %>%
      set_caption("Trends in Onset of Heat-Related Risk, by Country") %>%
      set_header_labels(values = c("Country", "Trend [C°/10 years]")) %>%
      autofit()

  # interpolate regression models and document mean t_zero by country and by age
  df_tzero_tmin <- do_models_1 %>% select(country = c, geo = g, age = a, sex = s, year = y) %>% distinct() %>%
    group_by(country, geo, age, sex) %>% reframe(year = median(year)) %>%
    mutate(t_zero = predict(lm_zero, newdata = .), t_min = predict(lm_min, newdata = .)) %>% select(!year)
  df_tzero_tmin %>%
    mutate(age_sex = paste(age, sex, sep = ":")) %>%
    group_by(Country = country, age_sex) %>% reframe(t_zero = round(median(t_zero), 1)) %>%   
    pivot_wider(values_from = t_zero, names_from = age_sex) %>%
    bind_rows(reframe(., across(where(is.factor), ~"Mean"), across(where(is.numeric), ~round(mean(.), 1)))) %>%
    flextable() %>%
      set_caption(sprintf("Onset of Heat-Related Risk, by Country, Age, and Sex (Median across Regions, Linear Regression across Years)")) %>%
      align(align = c("left", rep("center", 8)), part = "all") %>% hline(i = 10) %>% autofit()

  # reload geography info file
  load(file = "../Processed/Geography.Rdata")

  # visualize onset of temperature risk (zero crossing) on map, by age by sex
  plot_pdf_png("Figure_3", aspect_ratio = 0.5,
    df_tzero_tmin %>%
    left_join(., geo_info, join_by("geo" == "id_region")) %>% st_as_sf() %>%
    ggplot() +
      geom_sf(aes(fill = t_zero)) +
      labs(fill = "T [°C]", y = "© EuroGeographics for the administrative boundaries") +
      scale_fill_gradientn(colors = safe_heatmap_palette, breaks = seq(15, 30, 5), limits = c(13.0, 32.0)) +
      geom_sf(inherit.aes = FALSE, aes(), linewidth = 0.05, color = "black", fill = NA) +
      std_theme() + theme(legend.position = "right") + facet_grid(vars(sex), vars(age)))
  print("Onset of Heat-Related Risk, Regression Fit across Years")

  # visualize minima in temperature response on map, by age by sex
  plot_pdf_png("Figure_S7", aspect_ratio = 0.5,
    df_tzero_tmin %>%
    left_join(., geo_info, join_by("geo" == "id_region")) %>% st_as_sf() %>%
    ggplot() +
      geom_sf(aes(fill = t_min)) +
      labs(fill = "T [°C]", y = "© EuroGeographics for the administrative boundaries") +
      scale_fill_gradientn(colors = safe_heatmap_palette, breaks = seq(15, 30, 5), limits = c(13.0, 32.0)) +
      geom_sf(inherit.aes = FALSE, aes(), linewidth = 0.05, color = "black", fill = NA) +
      std_theme() + theme(legend.position = "right") + facet_grid(vars(sex), vars(age)))
  print("Minima of Heat-Related Risk, Regression Fit across Years")

  # shift onset of heat-related risk to 0 °C by region, by age, by sex, with the most recent year as the fixed reference (2023)
  df_summer_shifted <- df_summer %>% left_join(., df_tzero_tmin, by = c("country", "geo", "age", "sex")) %>% mutate(across(starts_with("tmpc"), ~.x - t_zero))

  # save results from pass 1 (for quicker restart at the start of pass 2), merging England and Wales
  save(df_summer_shifted, file = "../Processed/SummerShifted.Rdata", compress = "xz")
```

## Pass 2 - Common model across regions and years, by age and by sex

Common GAMs are computed for the weekly death rates (assuming a gaussian error distribution) as a function of the mean temperature across the prior 0..3 days, separately by age groups and by sex, but across regions and years, for observed temperatures and for temperatures relative to the regional reference (onset of heat-related risks). Data from two COVID years (2020 and 2022) are excluded to avoid potential confounding by pandemic mortality effects during summer.

```{r fit_common_GAMs}
  # reload unshifted and shifted summer data file (can restart processing here)
  load(file = "../Processed/DeathsPopTemp.Rdata")
  load(file = "../Processed/SummerShifted.Rdata")

  # fit common models by age group, by sex, across regions and years
  do_models_2 <- df_summer_shifted %>% select(a = age, s = sex) %>% distinct()
  do_parallel <- min(nrow(do_models_2), get_num_cores())
  registerDoParallel(cores = do_parallel)
  blas_set_num_threads(get_num_procs() / do_parallel)

  response_functions <- foreach (i = 1:nrow(do_models_2), .combine = "rbind", .packages = c("tidyverse", "mgcv")) %dopar% {
    # extract subset of data for region, calculate weekly_death_rate for 100k population
    df_as <- df_summer %>% filter(age == do_models_2$a[i], sex == do_models_2$s[i]) %>%
      mutate(weekly_death_rate = 1.0e6 * n_deaths / n_pop) %>% filter(!year %in% COVID_YEARS)

    # fit common model across all geographies and years, accounting for differences in absolute rate, in secular trends, in recurring seasonal trends,
    # with a common smooth term for the average temperature over the prior 0..3 days with implicit summation over weekdays
    lag_0_3 <- as.matrix(df_as %>% select(starts_with("tmpc_lag_0_3_dow")))
    gam1 <- bam(weekly_death_rate ~ geo + country:factor(year) + ti(week, k = DOF_WEEK, bs = "ps", by = country) + ti(lag_0_3, k = DOF_TMPC, bs = "ps"),
                weights = n_pop, data = df_as, family = gaussian, discrete = TRUE)

    # predictions for steady state, where geo, country, year and week are ignored, but need to be provided
    df_grid <- data.frame(geo = "AT11", country = "Austria", year = max(YEARS_MODEL), week = 26, tmpc = seq(round(min(lag_0_3), 1), round(max(lag_0_3), 1), 0.1)) %>%
      mutate(lag_0_3 = tmpc)
    predicted_terms <- predict.gam(gam1, newdata = df_grid, type = "terms", terms = "ti(lag_0_3)", se.fit = TRUE)
    rm(gam1); gc()

    # response functions, by age and sex, to be aggregated
    observed <- cbind(tmpc = df_grid$tmpc,
          as.data.frame(predicted_terms$fit   ) %>% rename_with(~str_replace_all(.x, c("ti[(]lag_" = "response_", "[)]" = ""))),
          as.data.frame(predicted_terms$se.fit) %>% rename_with(~str_replace_all(.x, c("ti[(]lag_" = "se_", "[)]" = ""))))

    # extract subset of data for region, calculate weekly_death_rate for 100k population
    df_as <- df_summer_shifted %>% filter(age == do_models_2$a[i], sex == do_models_2$s[i]) %>%
      mutate(weekly_death_rate = 1.0e6 * n_deaths / n_pop) %>% filter(!year %in% COVID_YEARS)

    # fit common model across all geographies and years, accounting for differences in absolute rate, in secular trends, in recurring seasonal trends,
    # with a common smooth term for the average temperature over the prior 0..3 days with implicit summation over weekdays
    lag_0_3 <- as.matrix(df_as %>% select(starts_with("tmpc_lag_0_3_dow")))
    gam2 <- bam(weekly_death_rate ~ geo + country:factor(year) + ti(week, k = DOF_WEEK, bs = "ps", by = country) + ti(lag_0_3, k = DOF_TMPC, bs = "ps"),
                weights = n_pop, data = df_as, family = gaussian, discrete = TRUE)

    # predictions for steady state, where geo, country, year and week are ignored, but need to be provided
    df_grid <- data.frame(geo = "AT11", country = "Austria", year = max(YEARS_MODEL), week = 26, tmpc = seq(round(min(lag_0_3), 1), round(max(lag_0_3), 1), 0.1)) %>%
      mutate(lag_0_3 = tmpc)
    predicted_terms <- predict.gam(gam2, newdata = df_grid, type = "terms", terms = "ti(lag_0_3)", se.fit = TRUE)
    rm(gam2); gc()

    # response functions, by age and sex, to be aggregated
    corrected <- cbind(tmpc = df_grid$tmpc,
          as.data.frame(predicted_terms$fit   ) %>% rename_with(~str_replace_all(.x, c("ti[(]lag_" = "response_", "[)]" = ""))),
          as.data.frame(predicted_terms$se.fit) %>% rename_with(~str_replace_all(.x, c("ti[(]lag_" = "se_", "[)]" = ""))))
    
    # combine response functions
    rbind(observed  %>% mutate(model = "Observed Temperature"), corrected %>% mutate(model = "Temperature above Onset")) %>%
      mutate(age = do_models_2$a[i], sex = do_models_2$s[i])
  }

  # convert into tidy table
  response_functions <- response_functions %>%
    pivot_longer(!c(tmpc, age, sex, model)) %>%
    mutate(type = gsub("_.*$", "", name), lag = paste(gsub("^[a-zA-Z]*.", "Lag: ", name), "day(s)")) %>% select(!name) %>%
    pivot_wider(names_from = "type", values_from = "value")

  # visualize heat-related mortality: temperature response
  plot_pdf_png("Figure_4", aspect_ratio = 0.5,
    response_functions %>% arrange(age, sex) %>%
      ggplot(aes(x = tmpc, y = response, group_by = interaction(age, sex), fill = interaction(age, sex), color = interaction(age, sex))) +
        geom_ribbon(aes(ymin = response + qnorm(ALPHA * 0.5) * se, ymax = response + qnorm(1.0 - ALPHA * 0.5) * se), alpha = 0.2, color = NA) +
        geom_line() +
        labs(y = "Weekly Mortality Rate [1/100.000], with 95%-CI", x = "T [°C]") +
        scale_color_manual(values = safe_colorblind_palette, name = "Age.Sex") +
        scale_fill_manual(values = safe_colorblind_palette, name = "Age.Sex") +
        std_theme() + facet_wrap(~model, nrow = 1, scale = "free_x"))
  print("Heat-Related Response, Common Models, by Sex by Age")

  # compute and report power-law fits
  power_laws <- foreach (i = 1:nrow(do_models_2), .combine = "rbind", .packages = c("tidyverse", "mgcv")) %dopar% {
    nls1 <- nls(response ~ a + b * tmpc^c, start = list(a = 0.0, b = 1.0, c = 2.0),
      data = response_functions %>% filter(model == "Temperature above Onset", tmpc > 0.0, age == do_models_2$a[i], sex == do_models_2$s[i]))
    data.frame(power = coef(nls1)["c"], power_cl = confint(nls1, "c", level = 1.0 - ALPHA)[1], power_ch = confint(nls1, "c", level = 1.0 - ALPHA)[2]) %>%
      mutate(age = do_models_2$a[i], sex = do_models_2$s[i])
  }
  power_laws %>% mutate(power_ci = sprintf("%.2f [%.2f, %.2f]", power, power_cl, power_ch)) %>%
    select(Age = age, Sex = sex, Power = power_ci) %>% arrange(Age, Sex) %>%
    flextable() %>% set_caption("Power Law Approximation to Exposure Respose Curves (t >= t_zero)") %>% autofit()
```

## Pass 3 - Load Factors and Trends, GAM followed by LM

The common GAMs from pass 2 are re-computed for the weekly death rates (assuming a gaussian error distribution) as a function of the mean temperature across the prior 0..3 days, separately by age groups and by sex, but across regions and years, only for temperatures relative to the regional reference (onset of heat-related risks, by region, by age, by sex). Using the common response (by age, by sex) across all countries, regions, and years, the corresponding load factors and their annual trends are computed for each country.

Data from two COVID years (2020 and 2022) are excluded to avoid potential confounding by pandemic mortality effects during summer.

```{r fit_common_loading_models}
  # reload the summer data file with temperature exposure relative to the risk onset (can restart processing here)
  load(file = "../Processed/SummerShifted.Rdata")

  # fit common models by age group, by sex, across regions and years, exclude youngest age groups that showed little or no dependency (flat)
  do_models_3 <- df_summer_shifted %>% select(a = age, s = sex) %>% distinct() %>% filter(a != "0-64")
  do_parallel <- min(nrow(do_models_3), get_num_cores())
  registerDoParallel(cores = do_parallel)
  blas_set_num_threads(get_num_procs() / do_parallel)

  load_functions <- foreach (i = 1:nrow(do_models_3), .combine = "rbind", .packages = c("tidyverse", "mgcv")) %dopar% {
    # extract subset of data for region, calculate weekly_death_rate for 100k population
    df_as <- df_summer_shifted %>% filter(age == do_models_3$a[i], sex == do_models_3$s[i]) %>%
      mutate(weekly_death_rate = 1.0e6 * n_deaths / n_pop) %>% filter(!year %in% COVID_YEARS) %>%
      mutate(year = year - LOADS_YEAR) # reference year for factor comparison plot
 
    # re-fit common model across all geographies and years, accounting for differences in absolute rate, in secular trends, in recurring seasonal trends,
    # with a common smooth term for the average temperature over the prior 0..3 days with implicit summation over weekdays
    lag_0_3 <- as.matrix(df_as %>% select(starts_with("tmpc_lag_0_3_dow")))
    gam3 <- bam(weekly_death_rate ~ 0 + geo + country:factor(year) + ti(week, k = DOF_WEEK, bs = "ps", by = country) + ti(lag_0_3, k = DOF_TMPC, bs = "ps"),
                weights = n_pop, data = df_as, family = gaussian, discrete = TRUE)

    # append response functions, subtracting out the offset at the onset temperature
    df_as <- df_as %>% mutate(t_response = predict.gam(gam3) - predict.gam(gam3, newdata = df_as %>% mutate(lag_0_3 = 0.0)))

    # extract load functions from a linear model, using the common response as a predictor variable, replacing ti(week) by bs(week)
    # and computing country level deviations from the mean response, specified as the offset
    lm1 <- lm(weekly_death_rate ~ 0 + geo + country:factor(year) + country:bs(week, df = DOF_WEEK) + offset(t_response) + country:t_response + country:year:t_response,
              weights = n_pop, data = df_as)
    coefs1 <- as.data.frame(summary(lm1)$coef) %>% mutate(var = rownames(.)) %>% filter(grepl("t_response", var))
    row.names(coefs1) <- NULL
    coefs1 %>% mutate(age = do_models_3$a[i], sex = do_models_3$s[i])
  }

  # extract results from load functions
  results <- load_functions %>% mutate(Type = ifelse(grepl("year", var), "Trend", "Scale")) %>%
    mutate(country = str_replace_all(var, c("country" = "", ":t_response" = "", ":year" = ""))) %>%
    select(Type, country, age, sex, value = Estimate, SE = "Std. Error", p.value = "Pr(>|t|)")

  # document significant deviations and trends
  results %>% mutate(Type = paste(Type, ifelse(value < 0.0, "-", "+"), ifelse(p.value < ALPHA, "*", ""))) %>%
    group_by(Country = country, Type) %>% reframe(N = n()) %>%
    pivot_wider(names_from = "Type", values_from = "N") %>%
    mutate(across(everything(), ~ifelse(is.na(.), 0, .))) %>%
    select(Country, "Scale - *", "Scale + *", "Trend - *", "Trend + *") %>%
    bind_rows(reframe(., across(where(is.character), ~"Total"), across(where(is.numeric), sum))) %>%
    mutate(across(is.numeric, ~ifelse(. != 0, ., ""))) %>%
    flextable() %>%
      set_caption("Significant Scales (!= 1) and Trends (!= 0) for Heat Sensitivity by Country: Number of Age/Sex Groups") %>%
      hline(i = 10) %>% align(align = c("left", rep("center", 4))) %>% autofit()

  # reload geography info file
  load(file = "../Processed/Geography.Rdata")

  # aggregate regions to country level
  country_info <- geo_info %>% mutate(country = ifelse(country %in% c("England", "Wales"), "England.Wales", country)) %>%
    group_by(country) %>% reframe(combined_geometry = st_union(geometry))

  # visualize heat sensitivity results
  plot_pdf_png("Figure_5", aspect_ratio = 0.7,
    results %>% filter(Type == "Scale") %>%
    mutate(Signif = ifelse(p.value < ALPHA, "*", ""), value = value + 1.0) %>%
    left_join(., country_info, by = "country") %>% st_as_sf() %>%
    ggplot() +
      geom_sf(aes(geometry = combined_geometry, fill = value)) +
      geom_sf_text(inherit.aes = FALSE, aes(label = Signif), size = 5.0) +
      scale_fill_gradient2(low = "#007F00", mid = "#FFFFFF", high = "#FF0000", midpoint = 1.0) +
      labs(fill = "Scale", x = "© EuroGeographics for the administrative boundaries") +
      std_theme() + theme(legend.position = "right", legend.text = element_text(hjust = 1)) + facet_grid(vars(sex), vars(age)))
  print(sprintf("Heat Sensitivity (%d): Scale x Common Age-Sex Response (across Countries and %d-%d), * = (p < %.2f)",
                LOADS_YEAR, min(YEARS_MODEL), max(YEARS_MODEL), ALPHA))

  # visualize heat sensitivity trend results
  plot_pdf_png("Figure_6", aspect_ratio = 0.7,
    results %>% filter(Type == "Trend") %>%
    mutate(Signif = ifelse(p.value < ALPHA, "*", "")) %>%
    left_join(., country_info, by = "country") %>% st_as_sf() %>%
    ggplot() +
      geom_sf(aes(geometry = combined_geometry, fill = value)) +
      geom_sf_text(inherit.aes = FALSE, aes(label = Signif), size = 5.0) +
      scale_fill_gradient2(low = "#007F00", mid = "#FFFFFF", high = "#FF0000", midpoint = 0.0) +
      labs(fill = "Annual\nScale\nChange\n[1/year]", x = "© EuroGeographics for the administrative boundaries") +
      std_theme() + theme(legend.position = "right", legend.text = element_text(hjust = 1)) + facet_grid(vars(sex), vars(age)))
  print(sprintf("Heat Sensitivity Trend (%d-%d): Slope = Annual Change to Factor, * = (p < %.2f)",
                min(YEARS_MODEL), max(YEARS_MODEL), ALPHA))
```

## Done

```{r}
  print(Sys.time() -start_time)
```